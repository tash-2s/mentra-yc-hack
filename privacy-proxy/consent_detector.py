# consent_detector.py
import asyncio
import json
import logging
import os
import shutil
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional, List, Tuple

import cv2
import numpy as np
from openai import AsyncOpenAI

logger = logging.getLogger("consent-detector")


class ConsentDetector:
    """Monitors transcript files and detects recording consent using OpenAI API"""

    def __init__(self, transcript_dir: Path = Path("./transcripts"),
                 check_interval: float = 3.0,
                 api_key: Optional[str] = None,
                 capture_dir: Path = Path("./captures"),
                 consent_capture_dir: Path = Path("./consented_captures"),
                 crop_faces: bool = True):
        self.transcript_dir = transcript_dir
        self.check_interval = check_interval
        self.capture_dir = capture_dir
        self.consent_capture_dir = consent_capture_dir
        self.crop_faces = crop_faces
        self.running = False
        self._last_processed_content = None
        self._last_file_path = None
        self.face_detector = None

        # Initialize OpenAI client
        api_key = api_key or os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not found. Consent detection will not work.")
            logger.warning("Set the OPENAI_API_KEY environment variable to enable this feature.")

        self.client = AsyncOpenAI(api_key=api_key)
        
        # Initialize YuNet face detector if face cropping is enabled
        if self.crop_faces:
            try:
                model_path = Path("face_detection_yunet_2023mar.onnx")
                if model_path.exists():
                    self.face_detector = cv2.FaceDetectorYN_create(  # type: ignore
                        model=str(model_path),
                        config="",
                        input_size=(320, 320),
                        score_threshold=0.6,
                        nms_threshold=0.3,
                        top_k=5000
                    )
                    logger.info("YuNet face detector initialized for consent image cropping")
                else:
                    logger.warning(f"YuNet model not found at {model_path}. Face cropping disabled.")
                    self.crop_faces = False
            except Exception as e:
                logger.error(f"Failed to initialize YuNet face detector: {e}")
                self.crop_faces = False

        logger.info(f"ConsentDetector initialized. Monitoring: {self.transcript_dir}")

    def get_latest_transcript_file(self) -> Optional[Path]:
        """Find the most recent transcript file"""
        if not self.transcript_dir.exists():
            return None

        transcript_files = list(self.transcript_dir.glob("transcript-*.txt"))
        if not transcript_files:
            return None

        # Sort by filename (contains timestamp), get the most recent
        return sorted(transcript_files)[-1]

    def read_last_n_lines(self, file_path: Path, n: int = 3) -> List[str]:
        """Read the last n lines from a file efficiently"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                return lines[-n:] if len(lines) >= n else lines
        except Exception as e:
            logger.error(f"Error reading transcript file: {e}")
            return []

    async def check_consent(self, transcript_lines: List[str]) -> Optional[dict]:
        """Check for consent in the transcript lines using OpenAI API"""
        if not transcript_lines:
            return None

        # Join lines with newlines for better context
        transcript_text = "\n".join(transcript_lines)

        try:
            response = await self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": """Analyze the following transcript excerpt to determine if someone has given consent to be recorded.

Note: This transcript is automatically generated by speech recognition and may contain transcription errors.

Look for explicit consent statements like:
- "i consent to be recorded"
- "you can record me"
- "i agree to recording"
- "i give permission to being captured on camera"
- "you have my permission to record"
- "yes, you can record this"

Important: If multiple consent statements are found in the transcript, use the LATEST (most recent) one based on the timestamps.

Also identify the person's name if mentioned.
Extract the timestamp from the transcript if consent is given (use the timestamp of the latest consent if multiple are found)."""
                    },
                    {
                        "role": "user",
                        "content": f"Transcript excerpt:\n{transcript_text}"
                    }
                ],
                response_format={
                    "type": "json_schema",
                    "json_schema": {
                        "name": "consent_detection",
                        "schema": {
                            "type": "object",
                            "properties": {
                                "consented": {
                                    "type": "boolean",
                                    "description": "Whether explicit consent to recording was given"
                                },
                                "individual_name": {
                                    "type": ["string", "null"],
                                    "description": "Name of the person who gave consent, if mentioned"
                                },
                                "consent_timestamp": {
                                    "type": ["string", "null"],
                                    "description": "Timestamp when consent was given, extracted from transcript in format YYYY-MM-DD-HH-MM-SS"
                                }
                            },
                            "required": ["consented"],
                            "additionalProperties": False
                        }
                    }
                },
                temperature=0.1  # Low temperature for consistent detection
            )

            content = response.choices[0].message.content
            if content is None:
                logger.error("OpenAI API returned empty content")
                return None
            return json.loads(content)

        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            return None

    def format_log_message(self, result: dict) -> str:
        """Format the consent detection result for logging"""
        timestamp = datetime.now(tz=timezone.utc).strftime("%Y-%m-%d-%H-%M-%S")

        if result["consented"]:
            msg = f"[{timestamp}] Consent detected: Yes"
            if result.get("individual_name"):
                msg += f" | Name: {result['individual_name']}"
            if result.get("consent_timestamp"):
                msg += f" | Time: {result['consent_timestamp']}"
        else:
            msg = f"[{timestamp}] Consent status: No"

        return msg
    
    def parse_timestamp_from_filename(self, filename: str) -> Optional[datetime]:
        """Parse timestamp from capture filename (YYYY-MM-DD-HH-MM-SS.jpg format)"""
        try:
            # Remove .jpg extension
            name = filename.replace('.jpg', '')
            
            # Parse YYYY-MM-DD-HH-MM-SS format
            parts = name.split('-')
            if len(parts) == 6:
                year, month, day, hour, minute, second = parts
                dt = datetime(int(year), int(month), int(day),
                             int(hour), int(minute), int(second),
                             tzinfo=timezone.utc)
                return dt
        except Exception as e:
            logger.debug(f"Failed to parse timestamp from {filename}: {e}")
        return None
    
    def find_capture_image(self, consent_timestamp: str, margin_seconds: int = 2) -> Optional[Path]:
        """Find the closest capture image within the time margin"""
        try:
            # Parse consent timestamp (YYYY-MM-DD-HH-MM-SS format)
            parts = consent_timestamp.split('-')
            if len(parts) != 6:
                logger.warning(f"Invalid consent timestamp format: {consent_timestamp}")
                return None
                
            year, month, day, hour, minute, second = map(int, parts)
            consent_dt = datetime(year, month, day, hour, minute, second, tzinfo=timezone.utc)
            
            # Define time window
            start_time = consent_dt - timedelta(seconds=margin_seconds)
            end_time = consent_dt + timedelta(seconds=margin_seconds)
            
            # Find all capture files
            if not self.capture_dir.exists():
                logger.warning(f"Capture directory not found: {self.capture_dir}")
                return None
                
            capture_files = list(self.capture_dir.glob("*.jpg"))
            
            # Find files within time window
            candidates: List[Tuple[Path, datetime, float]] = []
            
            for file in capture_files:
                file_dt = self.parse_timestamp_from_filename(file.name)
                if file_dt and start_time <= file_dt <= end_time:
                    # Calculate time difference in seconds
                    diff = abs((file_dt - consent_dt).total_seconds())
                    candidates.append((file, file_dt, diff))
            
            if not candidates:
                logger.info(f"No capture images found within {margin_seconds}s of {consent_timestamp}")
                return None
            
            # Sort by time difference and get the closest
            candidates.sort(key=lambda x: x[2])
            best_file, best_dt, diff = candidates[0]
            
            logger.info(f"Found capture image: {best_file.name} ({diff:.1f}s from consent time)")
            return best_file
            
        except Exception as e:
            logger.error(f"Error finding capture image: {e}")
            return None
    
    async def copy_consent_image(self, source_image: Path, person_name: Optional[str], 
                                consent_timestamp: str) -> Optional[Path]:
        """Detect main face and save cropped consent image"""
        try:
            # Load the image
            img = cv2.imread(str(source_image))
            if img is None:
                logger.error(f"Failed to load image: {source_image}")
                return None
                
            # If face cropping is disabled or detector not available, skip saving
            if not self.crop_faces or self.face_detector is None:
                logger.info(f"Face detection disabled, skipping consent image save")
                return None
                
            # Detect faces
            h, w = img.shape[:2]
            self.face_detector.setInputSize((w, h))  # type: ignore
            _, faces = self.face_detector.detect(img)  # type: ignore
            
            if faces is None or len(faces) == 0:
                logger.info(f"No face detected in {source_image}, skipping consent image save")
                return None
                
            # Select the main face
            main_face = self.select_main_face(faces, img.shape)
            if main_face is None:
                logger.info(f"No suitable face found in {source_image}, skipping consent image save")
                return None
                
            # Crop the face with padding
            cropped_face = self.crop_face_with_padding(img, main_face)
            if cropped_face is None:
                logger.warning(f"Failed to crop face from {source_image}")
                return None
                
            # Create consent capture directory if needed
            self.consent_capture_dir.mkdir(parents=True, exist_ok=True)
            
            # Generate filename
            name = person_name if person_name else "anonymous"
            name = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_')).strip()
            name = name.replace(' ', '_')
            dest_filename = f"{consent_timestamp}_{name}.jpg"
            dest_path = self.consent_capture_dir / dest_filename
            
            # Save the cropped face with high quality
            cv2.imwrite(str(dest_path), cropped_face, [cv2.IMWRITE_JPEG_QUALITY, 95])
            logger.info(f"Saved cropped consent face to: {dest_path}")
            
            return dest_path
            
        except Exception as e:
            logger.error(f"Error processing consent image: {e}")
            return None
    
    def select_main_face(self, faces: np.ndarray, img_shape: Tuple[int, int]) -> Optional[np.ndarray]:
        """Select the main face using hybrid scoring: area - λ*distance²"""
        if faces is None or len(faces) == 0:
            return None
            
        h, w = img_shape[:2]
        img_center_x, img_center_y = w / 2, h / 2
        
        # λ (lambda) balances area vs position - 2.0 works well for most cases
        lambda_weight = 2.0
        
        best_score = -float('inf')
        best_face = None
        
        for face in faces:
            x, y, face_w, face_h = face[:4].astype(int)
            
            # Skip very small faces (less than 40px)
            if face_w < 40 or face_h < 40:
                continue
                
            # Calculate area
            area = face_w * face_h
            
            # Calculate distance squared from face center to image center
            face_center_x = x + face_w / 2
            face_center_y = y + face_h / 2
            distance_squared = (face_center_x - img_center_x) ** 2 + (face_center_y - img_center_y) ** 2
            
            # Hybrid score: area - λ*distance²
            score = area - lambda_weight * distance_squared
            
            if score > best_score:
                best_score = score
                best_face = face
                
        return best_face
    
    def crop_face_with_padding(self, img: np.ndarray, face_bbox: np.ndarray, 
                               output_size: int = 256, padding_ratio: float = 0.5) -> Optional[np.ndarray]:
        """Crop face with padding and return in align_faces.py format (256x256 with gray padding)"""
        try:
            x, y, w, h = face_bbox[:4].astype(int)
            img_h, img_w = img.shape[:2]
            
            # Add generous padding (50% like align_faces.py for hair and outline)
            pad_x = int(w * padding_ratio)
            pad_y = int(h * padding_ratio)
            
            # Calculate padded bounds
            x1 = max(0, x - pad_x)
            y1 = max(0, y - pad_y)
            x2 = min(img_w, x + w + pad_x)
            y2 = min(img_h, y + h + pad_y)
            
            # Crop the face region
            face_crop = img[y1:y2, x1:x2]
            
            if face_crop.size == 0:
                return None
                
            # Resize while maintaining aspect ratio
            crop_h, crop_w = face_crop.shape[:2]
            if crop_h > crop_w:
                new_h = output_size
                new_w = int(crop_w * (output_size / crop_h))
            else:
                new_w = output_size
                new_h = int(crop_h * (output_size / crop_w))
                
            # Ensure we don't exceed output size
            if new_w > output_size:
                new_w = output_size
                new_h = int(crop_h * (output_size / crop_w))
            if new_h > output_size:
                new_h = output_size
                new_w = int(crop_w * (output_size / crop_h))
                
            resized = cv2.resize(face_crop, (new_w, new_h), interpolation=cv2.INTER_CUBIC)
            
            # Create final image with gray padding (matching align_faces.py)
            final = np.full((output_size, output_size, 3), 128, dtype=np.uint8)
            y_offset = (output_size - new_h) // 2
            x_offset = (output_size - new_w) // 2
            final[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
            
            return final
            
        except Exception as e:
            logger.error(f"Error cropping face: {e}")
            return None

    async def monitor_loop(self):
        """Main monitoring loop that runs every check_interval seconds"""
        self.running = True
        logger.info(f"Starting consent monitoring (checking every {self.check_interval}s)")

        while self.running:
            try:
                # Find latest transcript file
                latest_file = self.get_latest_transcript_file()

                if not latest_file:
                    logger.debug("No transcript files found")
                    await asyncio.sleep(self.check_interval)
                    continue

                # Read last 3 lines
                lines = self.read_last_n_lines(latest_file, n=3)

                if not lines:
                    logger.debug("No content in transcript file")
                    await asyncio.sleep(self.check_interval)
                    continue

                # Check if all lines are empty or whitespace
                if all(not line.strip() for line in lines):
                    logger.debug("Transcript file contains only empty lines")
                    await asyncio.sleep(self.check_interval)
                    continue

                # Check if content has changed
                current_content = "\n".join(lines)

                if (self._last_file_path == latest_file and
                    self._last_processed_content == current_content):
                    logger.debug("No new content since last check")
                    await asyncio.sleep(self.check_interval)
                    continue

                # New content detected, check for consent
                logger.debug(f"Checking consent for new content from {latest_file.name}")
                result = await self.check_consent(lines)

                if result:
                    # Log the result
                    log_msg = self.format_log_message(result)
                    logger.info(log_msg)
                    
                    # If consent was detected, find and copy the corresponding image
                    if result.get("consented") and result.get("consent_timestamp"):
                        # Only try to find images if capture directory exists
                        if self.capture_dir.exists():
                            capture_image = self.find_capture_image(result["consent_timestamp"])
                            if capture_image:
                                await self.copy_consent_image(
                                    capture_image,
                                    result.get("individual_name"),
                                    result["consent_timestamp"]
                                )
                            else:
                                logger.info("No capture image found for consent timestamp")
                        else:
                            logger.debug("Capture directory not found, skipping image capture for consent")

                    # Update cache
                    self._last_file_path = latest_file
                    self._last_processed_content = current_content
                else:
                    logger.warning("Failed to get consent detection result")

            except Exception as e:
                logger.error(f"Error in monitoring loop: {e}", exc_info=True)

            await asyncio.sleep(self.check_interval)

        logger.info("Consent monitoring stopped")

    def stop(self):
        """Stop the monitoring loop"""
        logger.info("Stopping consent detector...")
        self.running = False
